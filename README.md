# RAG
## What is RAG?
### Definition
RAG (Retrieval-Augmented Generation) is an AI technique that combines retrieval and generation to improve the quality and accuracy of responses from large language models (LLMs). It enhances traditional LLMs by incorporating external knowledge retrieval before generating a response.

### Architecture

![image](https://github.com/user-attachments/assets/031a8ef3-3344-4838-b10f-a11699c935c9)

## Need for RAG
RAG (Retrieval-Augmented Generation) is essential because traditional LLMs have limitations in accuracy, knowledge freshness, and context-specific responses. RAG enhances AI models by integrating external retrieval with generation, making them more reliable and useful.

### Key Reasons for RAG's Need
#### Reduces Hallucinations (False Information)
LLMs sometimes generate incorrect or made-up answers (hallucinations).
RAG retrieves factual information from trusted sources, ensuring responses are grounded in reality.
#### Access to Up-to-Date Information
LLMs are trained on static datasets and do not update automatically.
RAG enables real-time retrieval from live sources (e.g., databases, websites, or internal documents).
#### Improved Accuracy & Relevance
Standard LLMs rely only on pre-trained knowledge and may miss domain-specific details.
RAG fetches relevant, verified data before generating a response, increasing reliability.
#### Better Domain-Specific Knowledge
Many industries (finance, legal, healthcare) need specialized and private knowledge.
RAG allows AI models to retrieve custom datasets instead of depending on general training data.
#### Efficient & Cost-Effective
Training large models from scratch is expensive and time-consuming.
RAG improves model performance without retraining, by integrating external retrieval.

### How RAG Works?
1. Retrieval: The model first searches for relevant information from external sources (e.g., a knowledge base, database, or document repository) based on the userâ€™s query.
2. Augmentation: The retrieved information is combined with the query and fed into the LLM.
3. Generation: The LLM generates a response using both the retrieved data and its pre-trained knowledge.


